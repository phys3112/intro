{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "262e3c4d",
   "metadata": {},
   "source": [
    "# Neural networks and machine learning\n",
    "\n",
    "## Aims:\n",
    "\n",
    "- to be able to perform complex tasks beyond simple things such as fitting straight lines to data. \n",
    "- Examples of such tasks are:\n",
    "\n",
    "  - handwriting recognition\n",
    "  - speech recognition\n",
    "  - image analysis (e.g., recognising objects in an image)\n",
    "  - predicting the weather from historical data, not from physics principles\n",
    "  - determining whether an email message is likely to be spam\n",
    "  - targetting advertising based on the data available about a person\n",
    "  - to answer \"google\" queries\n",
    "  - to be able to write coherent and accurate text (e.g., ChatGPT)\n",
    "  - finding patterns in very large datasets, e.g., examining 100's of billions of stars in our Galaxy, with dozens of data points (e.g., position, velocity, distance, spectrum) per star, and finding significant correlations (e.g., stars that were born at the same time/place; stars that come from an external galaxy)\n",
    "\n",
    "- to be able to think as well as, or better than, a human\n",
    "- to replace humans in many jobs\n",
    "- to replace humans\n",
    "\n",
    "<img src=\"https://mcba1.phys.unsw.edu.au/~mcba/t900.png\" width=\"300\" />\n",
    "\n",
    "## Approach\n",
    "\n",
    "- we have a model for a \"computer\" that can achieve at least some of the above aims moderately well: the human brain\n",
    "- the human brain derives its thinking ability primarily through the interaction of neurons and synapses (the connections between neurons)\n",
    "- if we simulate a neuron + synapses in a computer, and connect a bunch of them together, we should be able to think\n",
    "- aside: will a machine ever be able to truly be conscious and think?\n",
    "\n",
    "## How does a neuron work (greatly simplified)\n",
    "\n",
    "- a neuron is a single cell that can be treated as having a single output (the firing rate of an electrical pulse), and multiple inputs (from other neurons, though synapses)\n",
    "- the neuron's output is a function of summing all its inputs, with weights\n",
    "\n",
    "![A neuron](https://mcba1.phys.unsw.edu.au/~mcba/neuron.png)\n",
    "\n",
    "A complete map of the brain of a fruit fly: https://www.youtube.com/watch?v=NXr0ZdoYgRw with 3,016 neurons and 548,000 synapses, the result of a 12 year project completed in 2023.\n",
    "\n",
    "## An artificial neuron\n",
    "\n",
    "- is a _\"node\"_ with a single output and multiple inputs\n",
    "- the output is equal to the value of an _\"activation function\"_ that takes a single input number equals to a linear combination of the inputs with _\"weights\"_, and a _\"bias\"_ (an offset)\n",
    "- for example activation functions, see wikipedia\n",
    "\n",
    "## An artificial neural network\n",
    "\n",
    "![An artificial neural network, IBM](https://mcba1.phys.unsw.edu.au/~mcba/ibm-ann.png)\n",
    "\n",
    "- an artificial neural network is simply a collection of _\"nodes\"_\n",
    "- for convenience, the nodes are arranged in _\"layers\"_\n",
    "- there is an _\"input layer\"_ of nodes that receives input from outside the network, e.g., this might be\n",
    "     - an audio signal from a microphone\n",
    "     - the light intensity in a pixel of an image\n",
    "     - text\n",
    "- there is an _\"output layer\"_ of nodes that is the final result of the calculation, e.g.,\n",
    "     - the image contains a giraffe\n",
    "     - the email is spam with a certainly greater than 99%\n",
    "     - tomorrow's maximum temperature will be greater than 20C.\n",
    "- there are zero or more _\"hidden layers\"_ than embody the algorithm\n",
    "- a node can accept 1 or more inputs from nodes in the preceeding layer, and send its output to 1 or more nodes in the following layer\n",
    "  \n",
    "## To make a neural network useful you have to\n",
    "\n",
    "- specify the number of input nodes, the number of output nodes, and the number of hidden layers\n",
    "- define the inputs to the input layer\n",
    "- define the outputs from the output layer\n",
    "- specify the activation functions\n",
    "- set initial weights and biases for each node\n",
    "- train the network on data (i.e., find the weights and biases)\n",
    "- evaluate the performance of the network\n",
    "- if the performance isn't satisfactory, try changing training method, the node topology, the activation functions, perhaps improve the data quality, and retrain\n",
    "\n",
    "## The Google crash course on machine learning\n",
    "\n",
    "The following course is a 15 hour introduction to machine learning and the Google TensorFlow API:\n",
    "\n",
    "https://developers.google.com/machine-learning/crash-course/\n",
    "\n",
    "We are going to look at \"First Steps with TF\", and later \"Validation Set\".\n",
    "       \n",
    "## Machine learning terminology\n",
    "\n",
    "https://developers.google.com/machine-learning/glossary\n",
    "\n",
    "### Suppose we have a dataset upon which we want to train our model\n",
    "\n",
    "- a _\"feature\"_ in the dataset is analogous to a column of a spreadsheet, i.e., the value of some variable, such as time, temperature; it can be as complex as you like, e.g., the \"from\" line in an email, the number of words in an input textbox, the intensity of a star's spectrum at 543 nm\n",
    "- a _\"label\"_ is something we are trying to predict, e.g., \"this email is spam\", \"this star is of spectral type G2\", \"this image contains a giraffe\". The label is also a column in a spreadsheet, and there can be multiple labels for a feature\n",
    "- an _\"example\"_ is like a row of a spreadsheet;  a _\"labelled example\"_ has the label attached (perhaps by being manually entered by a human), an _\"unlabelled example\"_ has no label\n",
    "- a _\"model\"_ is the neural network with all its nodes, layers, activation functions, weights, and biases. It represents our way of estimating labels from features\n",
    "- _\"training\"_ the model is the process of refining the model so that it works to some desired accuracy\n",
    "- the _\"loss\"_ of a model is a measure of the quality of output from the model. A loss of zero is perfect, a high loss indicates a poor model\n",
    "- we have to define how we measure _\"loss\"_, and what function we might pass the raw loss measurement through to assist with training.  A common choice for loss is least squares, where we measure the loss by the average of the sum of the squares of the differences between the features and the labels (assumed to be scalars in this example)\n",
    "\n",
    "### We are now ready to train our model\n",
    "\n",
    "- the idea is to vary the _\"parameters\"_ (the weights and biases of the nodes), in such a way as to minimise the loss\n",
    "- the classic approach to doing this is _\"gradient descent\"_ to a minimum\n",
    "- we may need to use random jumps of parameters to cover sufficient phase space\n",
    "- for very large datasets we will have to train on a subset of the data\n",
    "- we may have to randomly select _*batches*_ of data from the main dataset in order to speed up the training process\n",
    "\n",
    "### \"hyperparameters\"\n",
    "\n",
    "- a _\"hyperparameter\"_ is some property of the model, and the learning process, beyond the weights and biases. Hyperparameters include the number of hidden layers, the number of nodes, the rate at which the model descends to a minimum, and the activation function\n",
    "- the loss function is differentiable with respect to the model _\"parameters\"_, which makes minimisation efficient with gradient descent; this isn't the case with hyperparameters \n",
    "- often, a human has to choose the hyperparameters, and let Tensorflow optimises the parameters\n",
    "\n",
    "### \"Feature engineering\"\n",
    "\n",
    "- the art/science of going from raw data to features\n",
    "- identifying and handling bad data\n",
    "- mapping data to features\n",
    "- scaling data so that the features map fairly uniformly to a range such as 0 to 1, or -1 to +1) \n",
    "- _\"one-hot encoding\"_ where a feature is a vector of binary values, and each example only has one bit set (e.g., the vector could represent every word in a dictionary, and the feature could be an input word)\n",
    "- _\"multi-hot encoding\"_ where multiple bits can be 1\n",
    "- note that the vectors for one-hot encoding are very sparse (i.e., almost all zeroes), so special techniques are used to store them without using lots of computer memory\n",
    "\n",
    "### Handling non-linear models\n",
    "\n",
    "- linear problems are very quick to train, so if at all possible, try to make the problem linear\n",
    "- one approach to linearising a model is to create _\"feature crosses\"_, i.e., creating a new feature by multiplying two or more features together\n",
    "\n",
    "### Training sets, validation sets, and test data sets\n",
    "\n",
    "- these are essential to avoid _\"overfitting\"_, where your model works perfectly with the data set, but fails when presented with new data\n",
    "- split your data into three subsets: the _\"training set\"_, the _\"validation\"_ set, and the _\"test set\"_. Choosing these subsets, and ensuring that they are all representative of the data, can be tricky. \n",
    "- train your model on the training set\n",
    "- evaluate the model on the validation set, then retrain the model\n",
    "- finally test your model on the test set\n",
    "\n",
    "### Regularization\n",
    "\n",
    "- _\"Regularization\"_ is simply reducing the complexity of a model to avoid overfitting. This is often done by setting some weights to zero. The complexity of a model can be quantified by the number of nonzero weights.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ed7dca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
